\documentclass{beamer}

% THEME (Metropolis)
\usetheme[numbering=counter,progressbar=foot]{metropolis} % clean + progress bar
\usepackage{appendixnumberbeamer}

% FONTS (works with pdfLaTeX; for nicer system fonts, compile with XeLaTeX)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% MATH / GRAPHICS
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\graphicspath{{figures/}}

% BIB (optional) — comment out if not needed
\usepackage[numbers]{natbib}
\usepackage{bibentry}

\usepackage{algorithm, algpseudocode}
\usepackage{float}

\usepackage{listings}
\usepackage{booktabs}        % for \toprule \midrule \bottomrule
\usepackage{xcolor} % for colors
\usepackage{adjustbox} % to scale only if needed
\definecolor{codebg}{RGB}{245,247,250}
\definecolor{codekw}{RGB}{0,90,160}
\definecolor{codecm}{RGB}{110,110,110}
\definecolor{codestr}{RGB}{160,60,60}
\definecolor{codenum}{RGB}{120,120,120}

\lstdefinestyle{nbody}{
  language=C,
  basicstyle=\footnotesize\ttfamily,    % smaller but still readable
  numbers=none,                          % ← no line numbers
  keywordstyle=\color{codekw}\bfseries,
  commentstyle=\color{codecm}\itshape,
  stringstyle=\color{codestr},
  backgroundcolor=\color{codebg},
  showstringspaces=false,
  tabsize=2,
  keepspaces=true,
  columns=fullflexible,
  breaklines=true, breakatwhitespace=true,
  aboveskip=2pt, belowskip=2pt,
  xleftmargin=0pt, framexleftmargin=0pt,
  frame=single, rulecolor=\color{black!10}, % subtle frame
}




% TITLE INFO
\titlegraphic{\includegraphics[width=0.35\textwidth]{black_unipv_logo_caption_right.png}}
\title{Scalability and Communication Overhead in Distributed N-Body Simulation using MPI on GCP}
\subtitle{\textit{Advanced Computer Architecture}}
\author{Claudio Guarrasi \\ \small{Department of Electrical, Computer and Biomedical Engineering} \\ \small{University of Pavia}}
\institute{}
\date{}
\begin{document}

% TITLE SLIDE
\maketitle

% AGENDA
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{Introduction}
\begin{itemize}
	\item N-Body problem is a well known problem in physics.
	\item Assumptions:
	\begin{itemize}
		\item Closed system containing $N$ point masses (bodies).
		\item No collisions.
		\item Gravitational interaction only $\Rightarrow \vec{F}_{i,j}=G\frac{m_{i} m_{j}}{\|\vec{r}_{i}-\vec{r}_{j}\|^3}(\vec{r}_{j}-\vec{r}_{i})$.
	\end{itemize}
	\item A closed-form solution does not exist\footnote{Douglas C. Heggie, \emph{The Classical Gravitational N-Body Problem}, arXiv:astro-ph/0503600 (2005).}.
	\item Numerical solution: solving equations of motion at each time step $k$.	
\end{itemize}
\end{frame}

\section{Physical Model}
\begin{frame}{Physical Model}
Underlying physics:
\begin{itemize}
	\item $\vec{F}_{i,j}=G\frac{m_{i} m_{j}}{\|\vec{r}_{i}-\vec{r}_{j}\|^3}(\vec{r}_{j}-\vec{r}_{i})$ (Gravitational universal law).
	\item $\Sigma\vec{F}_i = m_i\vec{a}_i$ (Newton's second law).
\end{itemize}
\begin{center}
	$\Downarrow$
\end{center}
\begin{itemize}
	\item $\vec{a}_i=\sum_{\substack{j=0 \\ j \neq i}}^{N-1}\left[G\frac{m_j}{\|\vec{r}_i-\vec{r}_j\|^3}(\vec{r}_j-\vec{r}_i)\right]$ (Acceleration of body $i$).
	\item $\vec{v}_i(t_k) = \vec{v}_i(t_{k-1}) + \int_{t_{k-1}}^{t_k} \vec{a}_i(\tau)\, d\tau \Rightarrow \vec{v}_{k+1}=\vec{v}_k+\vec{a}_k\Delta t$.
	\item $\vec{r}_i(t_k) = \vec{r}_i(t_{k-1}) + \int_{t_{k-1}}^{t_k} \vec{v}_i(\tau)\, d\tau \Rightarrow \vec{r}_{k+1}=\vec{r}_k+\vec{v}_k\Delta t$.
\end{itemize}
Direct method approach: expensive but more intuitive.
\end{frame}

\begin{frame}{Numerical Integration}
Not all numerical integration methods are good:
\begin{itemize}
	\item The exact velocity $v(t_{k+1})$ is: $v(t_k+\Delta t) = v(t_k) + a(t_k)\,\Delta t
+ \frac{\Delta t^2}{2} \frac{d^2 v}{dt^2}(\xi), \xi \in \left[ t_k,\, t_k+\Delta t \right]$
Step size $\Delta t$ weighs trade-off between accuracy and computational cost.
	\item Energy drift: $K_{TOT}(t_k) = \frac{1}{2} \sum_{i=0}^{N-1}m_iv_i^2(t_k)$
	\\Geometrical structure of the trajectories is not retained.
\end{itemize}
Symplectic methods prevent energy drift\footnote{R.~D.~Engle, “Monitoring energy drift with shadow Hamiltonians,” \emph{Journal of Computational Physics}, vol. 210, no. 2, pp. 371–384, 2005.} $\Rightarrow$ Semi-implicit Euler method: $\vec{r}_{k+1}=\vec{r}_k+\textcolor{red}{\vec{v}_{k+1}} \, \Delta t$
\end{frame}

\section{Serial Algorithm}
\begin{frame}{Serial Algorithm (Direct Method)}
Conceptual design of the serial algorithm
\begin{algorithm}[H]
\caption{Serial N-body algorithm (direct method)}
\label{alg:serial}
\begin{algorithmic}[1]
\State Initialize positions, velocities, and masses
\For{each time step}
    \State Compute accelerations (Algorithm~\ref{alg:inner_loop})
    \State Update velocities
    \State Update positions
\EndFor
\end{algorithmic}
\end{algorithm}
Space complexity:
$N \ \text{masses} + 3N \ \text{positions} + 3N \ \text{velocities} + 3N \ \text{accelerations} = 10N \ \text{real values} \Rightarrow O(N)$ 
\end{frame}

\begin{frame}{Serial Algorithm (Direct Method)}
\begin{algorithm}[H]
\caption{Inner loop: compute accelerations (direct method)}
\label{alg:inner_loop}
\begin{algorithmic}[1]
\Require Positions $\{\vec r_j\}_{j=0}^{N-1}$, masses $\{m_j\}_{j=0}^{N-1}$
\Ensure Accelerations $\{\vec a_i\}_{i=0}^{N-1}$
\For{$i \gets 0$ to $N-1$}
  \State $\vec a_i \gets (0,0,0)$
  \For{$j \gets 0$ to $N-1$}
    \If{$j \neq i$}
      \State $\vec a_i \gets \vec a_i + \text{effect of body}\, j \, \text{on body}\, i$
    \EndIf
  \EndFor
\EndFor
\Statex \textbf{Time complexity:} $O(N^2)$
\end{algorithmic}
\end{algorithm}
\end{frame}




\begin{frame}[fragile]{C Implementation}{}
Each major step of the Serial Algorithm mapped to a dedicated function.

\begin{lstlisting}[style=nbody]
void computePositions(int bodies, double dt){
  for (int i=0; i<bodies; ++i)
    positions[i] += dt * velocities[i]; }
\end{lstlisting}

\vspace{2mm}

\begin{lstlisting}[style=nbody]
void computeAccelerations(int bodies) {
    const double epsilon = 1e-5;
    for (int i = 0; i < bodies; i++) {
        accelerations[i] = 0.0;
        for (int j = 0; j < bodies; j++) {
            if (i != j) {
                double dist = positions[j] - positions[i] + epsilon;
                double invDistCubed = 1.0 / (dist * dist * dist);
                double scalar = GravConstant * masses[j] * invDistCubed;
                accelerations[i] += scalar * dist;            }
        }
    }
}
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]{C Implementation: Profiling}
\centering

% --- Top row: two plots side by side ---
\begin{columns}[T,onlytextwidth]
  \column{0.5\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{logex_time_vs_logbodies.png}\\[1mm]
  {\footnotesize \textbf{Log–log:} execution time vs.\ bodies; power-law behavior.}

  \column{0.5\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{ex_time_vs_bodies.png}\\[1mm]
  {\footnotesize \textbf{Linear:} execution time as a function of $N$.}
\end{columns}

\vspace{3mm}
\hrule % horizontal line separating plots from tables
\vspace{3mm}

% --- Bottom row: two verbatim blocks side by side with vertical line ---
\noindent
\begin{minipage}[t]{0.48\textwidth}
{\tiny
\begin{verbatim}
gprof Flat profile:

Each sample counts as 0.01 seconds.

  %   self  seconds  calls   name
 98.49 150.54   100   computeAccelerations
  1.49   2.28     -   _init
  0.01   0.02   100   computeVelocities
  0.01   0.01   101   logPositions
  0.00   0.00 60000   rand_uniform
  0.00   0.00   100   computePositions
  0.00   0.00   100   simulate
  0.00   0.00     1   initiateSystem
\end{verbatim}
}
\end{minipage}%
\hfill
\vrule width0.5pt % vertical separator
\hfill
\begin{minipage}[t]{0.48\textwidth}
{\tiny
\begin{verbatim}
 perf counter stats:

 Task-clock (ms)          268.400
 CPU Utilization          1.000 CPUs

 Cycles                   1.165e9 (4.341 GHz)
 Stalled-cycles-frontend  1.590e6
                          (0.14% frontend idle)

 Instructions             2.837e9 (2.43 IPC)
 Branches                 1.152e8
 Branch-misses            7.05e7
                         (0.06% of all branches)
\end{verbatim}
}
\end{minipage}

\end{frame}

\begin{frame}{C Implementation (A-priori Study of Available Parallelism)}
\begin{itemize}
  \item Amdahl's law (theoretical speedup):
\end{itemize}

\[
Speedup(N) = \frac{S+P}{S + \tfrac{P}{N}}
\]

\begin{itemize}
  \item For \texttt{computeAcceleration}: no loop-carried dependency. \\
        $\Rightarrow S=0,\; P=1$
  \item $\Rightarrow \; Speedup(N) = N$
  \item Ideal case: assumes communication among processes is instantaneous.
\end{itemize}
\end{frame}

\section{MPI Parallel Implementation}

\begin{frame}{MPI Parallel Implementation (Conceptual Design)}
\begin{algorithm}[H]
\caption{Parallel N-body (single-process view)}
\label{alg:parallel}
\begin{algorithmic}[1]
\State Initialize local positions, local velocities, and local masses
\State Send local positions and local masses
\State Wait for global positions and global masses
\For{each time step}
    \State Compute local accelerations (Algorithm~\ref{alg:parallel_inner_loop})
    \State Update local velocities
    \State Update local positions
    \State Send local positions
    \State Wait for global positions
\EndFor
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}{MPI Parallel Implementation (Conceptual Design)}
\begin{algorithm}[H]
\caption{Parallel inner loop: compute local accelerations}
\label{alg:parallel_inner_loop}
\begin{algorithmic}[1]
\Require Global positions $\{\vec r_j\}_{j=0}^{N-1}$, global masses $\{m_j\}_{j=0}^{N-1}$
\Ensure Local accelerations $\{\vec a_i\}_{i=0}^{(N/P)-1}$
\For{each local body $i$}
  \State $\vec a_i \gets (0,0,0)$
  \For{$j \gets 0$ to $N-1$}
    \If{$j$ is not the same as global index of $i$}
      \State $\vec a_i \gets \vec a_i + \text{effect of body}\, j \, \text{on body}\, i$
    \EndIf
  \EndFor
\EndFor
\Statex \textbf{Time complexity (per process):} $O\!\left(\frac{N}{P} \cdot N\right) = O\!\left(\frac{N^2}{P}\right)$
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{MPI Parallel Implementation (MPI\_Allgatherv)}
\texttt{MPI\_Allgatherv} is invoked as:
\begin{lstlisting}[style=nbody]
// each process contributes local_positions (b elements)
MPI_Allgatherv(
    local_positions, b, MPI_VECTOR,
    global_positions, recvcounts, displs,
    MPI_VECTOR, MPI_COMM_WORLD);
\end{lstlisting}

\begin{itemize}
  \item \textbf{Flexibility:} each process can contribute a different number of items 
        (via \texttt{recvcounts}).
  \item \textbf{Consistent ordering:} data placed in the same global order 
        (via \texttt{displs} offsets).
  \item \textbf{Blocking:} no process continues until all have finished exchanging data.
\end{itemize}
\end{frame}



\begin{frame}[fragile]{MPI Parallel Implementation (Initialization and Testing)}
\begin{itemize}
  \item Random initialization: seed depends on the \textbf{global index} of each body.  
        $\Rightarrow$ mirrors the serial version’s indexing scheme.
  \item Enables a \textbf{direct comparison} of serial vs.\ parallel output.
\end{itemize}

\vspace{2mm}

\textbf{Compute global start index (per process):}
\begin{lstlisting}[style=nbody]
unsigned int global_start_index(int r, int bodies, int rank) {
    if (rank < r) {
        return rank * (bodies + 1);
    } else {
        return r * (bodies + 1) + (rank - r) * bodies;
    }
}
\end{lstlisting}

\textbf{Seed formula:}
\begin{lstlisting}[style=nbody]
unsigned int seed = start_index + i;
\end{lstlisting}
\end{frame}

\section{Performance Evaluation on GCP}
\begin{frame}{Performance Evaluation on GCP}
\centering
\textbf{Experimental configurations of virtual instances}
\vspace{0.4cm}

\begin{tabular}{l l}
\toprule
\textbf{Setup} & \textbf{Description} \\
\midrule
Single fat & 1 × 16 vCPUs, 32 GB RAM, 1 region \\
Inter-regional fat & 2 × 8 vCPUs, 16 GB RAM, 2 regions \\
Intra-regional thin & 4 × 3 vCPUs, 3 GB RAM, 1 region \\
Inter-regional thin & 4 × 3 vCPUs, 3 GB RAM, 2 regions \\
Four-region thin & 4 × 3 vCPUs, 3 GB RAM, 4 regions \\
\bottomrule
\end{tabular}
\end{frame}


\begin{frame}{Performance Evaluation on GCP (Strong Scaling)}
\begin{itemize}
\item Theoretical strong scalability ($N_1=N_P$):
\end{itemize}
\[
Scalability(P) = \frac{t_{parallel}(1)}{t_{parallel}(P)} = \frac{N_1^2}{N_p^2}P = P
\]
\begin{itemize}
	\item Equivalent to study speedup since there is no inherently sequential part in the serial algorithm.
	\item Near-linear speedup up to 16 processes on one VM.
\end{itemize}
\vspace{0.5em}
\centering
\includegraphics[width=0.5\linewidth]{parallel_strong_scaling_16_cores.png}
\end{frame}

\begin{frame}{Performance Evaluation on GCP (Strong Scaling)}
  \begin{itemize}
    \item Intra-regional clusters: negligible communication overhead.
    \item Inter-regional / multi-region: \texttt{MPI\_Allgatherv} becomes dominant.
  \end{itemize}
  \centering
  \includegraphics[width=0.85\linewidth]{communication_fraction_ex_time.png}
\end{frame}


\begin{frame}{Performance Evaluation on GCP (Weak Scaling)}
\begin{itemize}
	\item Theoretical weak scalability ($\tfrac{N_p}{P} = N_1$):
\end{itemize}
\[
Scalability(P) = \frac{t_{parallel}(1)}{t_{parallel}(P)} = \frac{N_1^2}{N_p^2}P = \frac{1}{P}
\]
\begin{itemize}
	\item The direct method inherently exhibits poor weak scalability, because each process must still loop over all $N$ bodies when computing accelerations.
\end{itemize}
  \centering
  \includegraphics[width=0.47\linewidth]{parallel_weak_scalability.png}
\end{frame}

\begin{frame}{Conclusions}
  \begin{itemize}
    \item Strong scalability holds intra-node / intra-region.
    \item Inter-region latency breaks scalability (blocking collective).
    \item Direct $O(N^2)$ method limits weak scalability.
    \item Future work: hierarchical/approximate methods (e.g., Barnes--Hut).
  \end{itemize}
\end{frame}

\end{document}

